"""
Vizota AI Celery Worker
ì´ë¯¸ì§€ íƒœê¹… ë° í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ Celery Worker
Redis ë©”ì‹œì§€ íë¥¼ ê°ì‹œí•˜ê³  ë¶„ì„ í›„ ë°±ì—”ë“œë¡œ ê²°ê³¼ ì „ì†¡
"""

import os
os.environ["TF_USE_LEGACY_KERAS"] = "1"
import logging
import requests
import json

from celery import Celery
from typing import List, Optional, Dict, Any

import torch
from transformers import MobileViTFeatureExtractor, MobileViTForImageClassification, pipeline
from PIL import Image

import predict_one_image

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Redis ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = os.getenv('REDIS_PORT', '6379')
REDIS_DB = os.getenv('REDIS_DB', '0')
REDIS_URL = f'redis://{REDIS_HOST}:{REDIS_PORT}/{REDIS_DB}'

# Celery ì•± ìƒì„±
app = Celery(
    'vizota_ai',
    broker=REDIS_URL,
    backend=REDIS_URL
)

# Celery ì„¤ì •
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Seoul',
    enable_utc=False,
    task_track_started=True,
    task_time_limit=300,  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=50,
    broker_connection_retry_on_startup=True,
)

# ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.cuda.is_available():
    device = torch.device('cuda')
    device_id = 0
elif torch.backends.mps.is_available():
    device = torch.device('mps')
    device_id = 0
else:
    device = torch.device('cpu')
    device_id = -1

logger.info(f"Using device: {device}")

# ì „ì—­ ë³€ìˆ˜ - ëª¨ë¸
feature_extractor = None
model = None
classifier = None
feature_maps = {}
target_layer_name = 'dropout'

# ë°±ì—”ë“œ API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥)
BACKEND_API_URL = os.getenv('BACKEND_API_URL', 'http://localhost:8000/api/images/{image_id}/analysis-results')


# Feature map hook
def get_features(name):
    def hook(model, input, output):
        if isinstance(output, tuple):
            feature_maps[name] = output[0].detach()
        else:
            feature_maps[name] = output.detach()
    return hook


# ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
def load_models():
    """Worker ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    global feature_extractor, model, classifier

    logger.info("ğŸš€ Loading AI models...")

    try:
        # MobileViT ëª¨ë¸ ë¡œë“œ
        feature_extractor = MobileViTFeatureExtractor.from_pretrained("apple/mobilevit-small")
        model = MobileViTForImageClassification.from_pretrained("apple/mobilevit-small")
        model.eval()
        model.to(device)

        # Feature extraction hook ì„¤ì •
        try:
            target_layer = dict(model.named_modules())[target_layer_name]
            target_layer.register_forward_hook(get_features(target_layer_name))
            logger.info("âœ… Feature Vector ì¶”ì¶œ ì„¤ì • ì™„ë£Œ")
        except KeyError:
            logger.warning("âš ï¸ Feature Vector ì¶”ì¶œ ì„¤ì • ì‹¤íŒ¨")

        # Zero-shot classification ëª¨ë¸ ë¡œë“œ
        classifier = pipeline(
            "zero-shot-classification",
            model="facebook/bart-large-mnli",
            device=device_id
        )

        logger.info(f"All models loaded successfully on {device}")

    except Exception as e:
        logger.error(f"Error loading models: {e}")
        raise


# Celery Worker ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
@app.task(bind=True)
def worker_init(self):
    """Worker ì´ˆê¸°í™” ì‹œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    load_models()
    return "Models loaded successfully"


# ë°±ì—”ë“œë¡œ ê²°ê³¼ ì „ì†¡
def send_result_to_backend(result_data: Dict[str, Any], task_id: str = None, image_id: str = None) -> bool:
    """
    ë¶„ì„ ê²°ê³¼ë¥¼ ë°±ì—”ë“œ APIë¡œ ì „ì†¡í•©ë‹ˆë‹¤.

    Args:
        result_data: ì „ì†¡í•  ê²°ê³¼ ë°ì´í„°
        task_id: Celery task ID (ì„ íƒ)

    Returns:
        bool: ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    try:
        headers = {'Content-Type': 'application/json'}

        # task_idê°€ ìˆìœ¼ë©´ ê²°ê³¼ ë°ì´í„°ì— í¬í•¨
        if task_id:
            result_data['task_id'] = task_id

        # image_idë¡œ URL ë™ì  ìƒì„±
        if image_id:
            api_url = BACKEND_API_URL.format(image_id=image_id)
        else:
            # image_idê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ URL ì‚¬ìš©
            api_url = BACKEND_API_URL.replace('/{image_id}', '')

        logger.info(f"ğŸ“¤ Sending result to backend: {api_url}")
        logger.info(f"ğŸ“Š Analysis Result Summary:")
        logger.info(f"   â€¢ Tag: {result_data.get('tag_name', 'N/A')} (probability: {result_data.get('probability', 0):.2f}%)")
        logger.info(f"   â€¢ Category: {result_data.get('category', 'N/A')} (probability: {result_data.get('category_probability', 0):.2f}%)")
        logger.info(f"   â€¢ Quality Score: {result_data.get('quality_score', 'N/A')}")
        logger.info(f"   â€¢ Feature Vector size: {len(result_data.get('feature_vector', []))}")
        logger.debug(f"ğŸ” Full result data: {json.dumps(result_data, indent=2)}")

        response = requests.post(
            api_url,
            json=result_data,
            headers=headers,
            timeout=30
        )

        response.raise_for_status()
        logger.info(f"âœ… Result sent successfully. Response: {response.status_code}")
        logger.info(f"ğŸ“¥ Backend response: {response.text[:200]}{'...' if len(response.text) > 200 else ''}")
        return True

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Failed to send result to backend: {e}")
        return False


# ì´ë¯¸ì§€ ë¶„ì„ Celery Task
@app.task(bind=True, name='app.tasks.analyze_image_task')
def analyze_image_task(
    self,
    image_url: str,
    candidate_labels: Optional[List[str]] = ['Landscape', 'Animal', 'City', 'People', 'Food'],
    image_id: Optional[str] = None,
    user_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Redis íë¡œë¶€í„° ì´ë¯¸ì§€ ë¶„ì„ ì‘ì—…ì„ ìˆ˜ì‹ í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        image_url: ë¶„ì„í•  ì´ë¯¸ì§€ì˜ S3 URL
        candidate_labels: (ì„ íƒ) ê³„ì¸µì  ë¶„ë¥˜ë¥¼ ìœ„í•œ í›„ë³´ ë ˆì´ë¸” ëª©ë¡
        image_id: (ì„ íƒ) ì´ë¯¸ì§€ ì‹ë³„ì
        user_id: (ì„ íƒ) ì‚¬ìš©ì ì‹ë³„ì

    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
            - tag_name: ì´ë¯¸ì§€ ë¶„ë¥˜ëª…
            - probability: ë¶„ë¥˜ í™•ë¥  (%)
            - category: ì¶”ì²œ ìƒìœ„ íƒœê·¸
            - category_probability: ì¶”ì²œ íƒœê·¸ í™•ë¥  (%)
            - quality_score: ì´ë¯¸ì§€ í’ˆì§ˆ ì ìˆ˜ (0-1)
            - feature_vector: ì¶”ì¶œëœ feature vector (1x640, list type)
    """
    try:
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë¡œë“œ
        if model is None or classifier is None:
            logger.info("Models not loaded. Loading now...")
            load_models()

        logger.info(f"[Task {self.request.id}] Analyzing image: {image_url}")
        if image_id:
            logger.info(f"Image ID: {image_id}")
        if user_id:
            logger.info(f"User ID: {user_id}")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        try:
            response = requests.get(image_url, verify=False, timeout=30)
            response.raise_for_status()
            from io import BytesIO
            image = Image.open(BytesIO(response.content)).convert('RGB')
            logger.info(f"âœ… Image downloaded successfully: {len(response.content)} bytes")
        except Exception as e:
            error_msg = f"Failed to load image: {str(e)}"
            logger.error(error_msg)
            raise Exception(error_msg)

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° íƒœê¹…
        inputs = feature_extractor(images=image, return_tensors="pt").to(device)

        with torch.no_grad():
            # íƒœê·¸ ì˜ˆì¸¡
            outputs = model(**inputs)
            logits = outputs.logits

            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            try:
                quality_score = predict_one_image.main(image)
                if torch.is_tensor(quality_score):
                    quality_score = quality_score.item()
            except Exception as e:
                logger.warning(f"Quality score calculation failed: {e}")
                quality_score = None

        # Top prediction ì¶”ì¶œ (Kê°’ ë³€ê²½ì„ í†µí•´ ì¶”ì²œ íƒœê·¸ ê°œìˆ˜ ë³€ê²½ ê°€ëŠ¥)
        top_probability, top_class_index = torch.topk(logits.softmax(dim=1) * 100, k=1)

        class_name = model.config.id2label[top_class_index[0][0].item()]
        # commaë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²« ë²ˆì§¸ íƒœê·¸ë§Œ ì¶”ì¶œ
        class_name = class_name.split(',')[0].strip()
        probability = top_probability[0][0].item()

        # Feature vector ì¶”ì¶œ
        feature_vector = None
        if target_layer_name in feature_maps:
            extracted_features = feature_maps[target_layer_name]
            feature_vector = extracted_features.cpu().numpy().tolist()
            logger.info(f"Feature vector size: {extracted_features.size()}")

        # ê³„ì¸µì  ë¶„ë¥˜
        recommended_high_tag = None
        recommended_high_tag_prob = None

        if candidate_labels and len(candidate_labels) > 0:
            try:
                hierar = classifier(class_name, candidate_labels, multi_label=True)
                recommended_high_tag = hierar['labels'][0]
                recommended_high_tag_prob = hierar['scores'][0] * 100
                logger.info(f"Recommended tag: {recommended_high_tag} ({recommended_high_tag_prob:.2f}%)")
            except Exception as e:
                logger.warning(f"Hierarchical classification failed: {e}")

        # ë°±ì—”ë“œ API í˜•ì‹ì— ë§ì¶° ê²°ê³¼ ìƒì„± (ImageAnalysisResult ìŠ¤í‚¤ë§ˆ)
        result = {
            'tag_name': class_name,
            'probability': round(probability, 2),  # íƒœê·¸ ì˜ˆì¸¡ í™•ë¥  (%)
            'category': recommended_high_tag if recommended_high_tag else 'Unknown',
            'category_probability': round(recommended_high_tag_prob, 2) if recommended_high_tag_prob else None,
            'quality_score': round(quality_score, 4) if quality_score else None,
            'feature_vector': feature_vector[0] if feature_vector else []  # ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ì„ë² ë”©
        }

        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ë¡œê¹…ìš©)
        result_metadata = {
            'probability_percent': round(probability, 2),
            'category_probability': round(recommended_high_tag_prob, 2) if recommended_high_tag_prob else None,
            'quality_score': round(quality_score, 4) if quality_score else None,
            'image_url': image_url,
        }

        if image_id:
            result_metadata['image_id'] = image_id
        if user_id:
            result_metadata['user_id'] = user_id

        logger.info(f"[Task {self.request.id}] Analysis complete: {class_name} ({probability:.2f}%)")

        # ë°±ì—”ë“œë¡œ ê²°ê³¼ ì „ì†¡
        send_success = send_result_to_backend(result, task_id=self.request.id, image_id=image_id)
        result['sent_to_backend'] = send_success

        return result

    except Exception as e:
        logger.error(f"[Task {self.request.id}] Error analyzing image: {e}")

        # ì—ëŸ¬ ì •ë³´ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡ (ImageAnalysisResult ìŠ¤í‚¤ë§ˆ í˜•ì‹)
        error_result = {
            'tag_name': 'error',
            'probability': 0.0,
            'category': 'Unknown',
            'category_probability': None,
            'quality_score': None,
            'feature_vector': []
        }

        send_result_to_backend(error_result, task_id=self.request.id, image_id=image_id)

        raise


if __name__ == "__main__":
    # Worker ì‹¤í–‰ ë°©ë²•:
    # celery -A server worker --loglevel=info --concurrency=1
    logger.info("Starting Vizota AI Celery Worker...")
    logger.info("Run with: celery -A server worker --loglevel=info --concurrency=1")
